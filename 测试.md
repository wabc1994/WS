# 测试
1. 测试环境Linux centos
2. 测试工具 webbench 网页测压工具
3. 关闭TCP Nagle算法（减少小包的发送数量，提供）
4. 分别测试长连接和短连接之间的两种情况
5. 观察指标，就是显示给我们的信息:Page/min:指输出的页数/分、bytes/sec: 是指比特/秒


# 为何要关闭tcp_nodelay
现在让我们假设某个应用程序发出了一个请求，希望发送小块数据，比如sns游戏中的点击确定按钮。我们可以选择立即发送数据或者等待产生更多的数据然后再一次发送两种策略。如果我们马上发送数据，那么交互性的以及客户/服务器型的应用程序将极大地受益。例如，当我们正在发送一个较短的请求并且等候较大的响应时，相关过载与传输的数据总量相比就会比较低，而且，如果请求立即发出那么响应时间也会快一些。以上操作可以通过设置套接字的TCP_NODELAY选项来完成，这样就禁用了Nagle算法，在nginx中设置tcp_nodelay on,注意放在http标签里。
   
   另外一种情况则需要我们等到数据量达到最大时才通过网络一次发送全部数据，这种数据传输方式有益于大量数据的通信性能，典型的应用就是文件服务器。应用Nagle算法在这种情况下就会产生问题。但是，如果你正在发送大量数据，你可以设置TCP_CORK选项禁用Nagle化，其方式正好同TCP_NODELAY相反（TCP_CORK 和 TCP_NODELAY 是互相排斥的）。下面就让我们仔细分析下其工作原理。 
      
 >Nagle算法用于对缓冲区内的一定数量的消息进行自动连接。该处理过程(称为Nagling)，通过减少必须发送的封包的数量，提高了网络应用 程序系统的效率. 但是会带来一定的延迟，所以我们得禁止掉他
 
# 修改Linux tcp/ip 协议tcp相关的参数


[大并发下listen的连接完成对列backlog太小导致客户超时，服务器效率低下 ](https://blog.csdn.net/lizhi200404520/article/details/6981272)

[How TCP backlog works in Linux](http://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html)

## man listen()
 // 服务器进行监听的 全连接队列情况
 
 ```java
 
 int socket_bind_listen
  
  // 虽然设置为2048 实质上最大只能为128个
 
    if(listen(listen_fd, 2048) == -1)
        return -1;
 ```
 
 
-  cat /proc/sys/net/ipv4/tcp_max_syn_backlog 1024 这是默认情况下值，这是半连接队列大小
-  net.ipv4.tcp_max_syn_backlog = 8192
表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。
-  全连接队列的默认大小是 cat /proc/sys/net/core/somaxconn 128


 
 **accept queue满了之后** 
 >listen 第二个backlog参数设置太小，原来上面这个服务器代码listen指定的backlog连接完成队列参数太小，只有32，导致高并发的时候，服务器的连接完成队列在极短的时间内被填满了，而accept的处理速度跟不上队列填满的速度，导致队列始终是满的，然后就不理会客户的其他连接请求，导致了客户connect超时，并且处理效率低下。
而线程池的backlog有1024，不过受限于内核参数的默认值最大128，所以线程池这个的backlog实际是128（见man listen），再加上300个线程，每个线程独自accpet，所以能很快从完成队列中取得连接，客户的connect也不会超时了，如果把线程数改为1个，客户连接也会超时。
 
 

***满了之后如果开启了tcp_syncookies机制***

当 SYN queue 满了，系统还在不断的收到 SYN 包时，系统怎么处理的？系统会根据内核参数 net.ipv4.tcp_syncookies 的值来处理请求。tcp_syncookies 是用来防止 SYN flood 攻击，其原理是在半连接队列满时，SYN cookies 并不丢弃 SYN 请求，而是将源目的 IP、源目的端口号、接收到的 client 端初始序列号以及其他一些安全数值等信息进行 hash 运算，并加密后得到 server 端的初始序列号，称之为 cookie。server 端在发送初始序列号为 cookie 的 SYN+ACK 包后，会将分配的连接请求块释放。如果接收到 client 端的 ACK 包，server 端将 client 端的 ACK 序列号减 1 得到的值，与上述要素 hash 运算得到的值比较，如果相等，直接完成三次握手，构建新的连接。SYN cookies 机制的核心就是避免攻击造成的大量构造无用的连接请求块，导致内存耗尽，而无法处理正常的连接请求。

当 syn queue 满了，在 Server 端没有开启 syncookies 的时候，即 syncookies=0，server 端会丢弃新来的 SYN 包，而 client 端在多次重发 SYN 包得不到响应而返回 connection time out 错误。但是，当 Server 端开启了 syncookies, 即 syncookies=1，那么 SYN queue 就在逻辑上的没有最大值了，而是忽略内核参数 net.ipv4.tcp_max_syn_backlog 设置的值。Client 端在多次重发 SYN 包得不到响应而返回 connection time out 错误。

注意，即使开启该机制并不意味着所有的连接都是用 SYN cookies 机制来完成连接的建立，只有在 SYN queue 已满的情况下才会触发 SYN cookies 机制。由于 SYN cookies 机制严重违背 TCP 协议，不允许使用 TCP 扩展，可能对某些服务造成严重的性能影响（如 SMTP 转发），对于防御 SYN flood 攻击的确有效。对于没有收到攻击的高负载服务器，不要开启此选项，可以通过修改 tcp_max_syn_backlog、tcp_synack_retries 和 tcp_abort_on_overflow 系统参数来调节。

[SYN Cookie的原理和实现 ](https://www.cnblogs.com/i0ject/p/3869231.html)


 
大并发下listen的连接完成对列backlog太小导致客户超时，服务器效率低下，因为我把backlog 参数设置为最大值2048, 默认是128


**SYN flood攻击**

半连接之后服务器端就为客户端分配数据结构、内存等资源，如果一只耳没收到客户端的ack，这些资源就一直不释放的，导致服务器资源耗尽，不能正常对外提供服务


**全连接队列的大小确定**

全连接队列的大小min(backlog,somaxconn)，SOMAXCONN 默认是128 ，backlog 是用户程序调用listen()指定，全连接队列和半连接队列是由用户应用程序和Linux系统内核来决定的

**半连接队列的大小**

max(tcp_max_syn_backlog,64） 其中Linux 内核当中tcp_max_syn_backlog 是1024 ，处于SYN_SEND和SYN_RECV的都是半连接状态的，半连接队列是由系统参数和Linux tcp/ip协议栈来解决的


# Linux tcp/ip协议栈参数调优



# 查看命令行
- ss -s 
- netstat -s| grep LISTEN 主要是查看全连接队列满了之后，服务器端丢弃客户端的syn，查看是否有溢出的SYN，查看服务器accept队列满了之后，客户单有发送过来的ack 确认包被丢弃了


 
 
# nagle算法
避免网络有大量的小数据包情况，
1. maximum segment size(最大报文长度)  tcp数据当中的应用程序层数据的最大大小 ，不包括协议头，一个数据从应用层下来，会添加上20字节的tcp头部， 到ip层的话还会加上20字节的ip协议头，MSS是TCP数据包每次能够传输的最大数据分段
2. mtu

# 如果服务器中包含大量一直处于close_wait的tcp连接
tcp当中是tcp四次挥手被动关闭连接那一方接受到客户端发送过来的FIN后

 tcp的挥手连接同样是通过socket建立连接来完成的
 
 ![](https://github.com/wabc1994/WS/commit/4433f7b0dfc4425e50dcda91a84b9664a9c61b06)

1. fin_wait1  发送一个fin 进入 停止等待状态1
2. fin_wait2  接受到服务器端的ack 接受到fin 结束fin_wait2
3. time_wait 接受到 fin 进入，发送ack 时间等待
4. closed    等待两个最长报文生存时间后进入

[TCP连接大量CLOSE_WAIT状态问题排查](https://blog.csdn.net/yangguosb/article/details/79095255)

被动方关闭
1. 接受到fin后进入close_wait,同时发送一个ack
1. close_wait 发送一个ack 给 
3. 服务器端发送一个fin 进入Last_ACK 状态
>因此从close_wait进入fin的前提条件是被动方没有给主动方发送fin
关于tcp四次挥手详细过程可以看书, 

**tcp的四次挥手会如果不能顺利完成会影响什么资源**
>因为我们知道tcp实质上利用socket 进行通信的，而socket在Linux系统当中是采取一个文件描述符来描述，因此影响的是最终的系统资源，比如导致打开的open files 过多等等情况的


查看一些固定参数可以使用 ulimit -s 或者ulimit -a


## 主动方关闭进入time_wait 可以同过优化内核参数来解决

优化四次握手主动方的性能请求主要是通过tcp 内核

1. 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭  
net.ipv4.tcp_syncookies = 1  
2. 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭  
net.ipv4.tcp_tw_reuse = 1  
3. 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭  
net.ipv4.tcp_tw_recycle = 1
4. 表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间  
net.ipv4.tcp_fin_timeout=30

## close_wait 是从服务器来解决，也就是我们的应用程序

进入大量的close_wait状态说明套接字是被动关闭了，Server 程序处于CLOSE_WAIT状态，而不是LAST_ACK状态，说明还没有发FIN给ClientServer 程序处于CLOSE_WAIT状态，而不是LAST_ACK状态，说明还没有发FIN给Client



